{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"wsd_data/\"\n",
    "def_ext = \".definition\"\n",
    "tst_ext = \".test\"\n",
    "stop_words = stopwords.words('english')\n",
    "print(sorted(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store all file names necessary\n",
    "def_files = []\n",
    "test_files = []\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(def_ext):\n",
    "        #print(file)\n",
    "        def_files.append(os.path.join(data_path,file))\n",
    "    elif file.endswith(tst_ext):\n",
    "        test_files.append(os.path.join(data_path,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    norm_text = None\n",
    "    #for text in texts:\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    list_words = text.split()\n",
    "    words_wo_stop = []\n",
    "    # Stop word Removal\n",
    "    for word in list_words:\n",
    "        if word not in stop_words:\n",
    "            # Stemming\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            word = stemmer.stem(word)\n",
    "            words_wo_stop.append(word)\n",
    "    text = \" \".join(words_wo_stop)\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub('['+string.punctuation+']', ' ', text)\n",
    "    text = re.sub('[·´]','',text)\n",
    "    text = re.sub('[0-9]','',text)\n",
    "\n",
    "    norm_text = text.split()\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading wsd_data/bass.definition\n",
      "Reading wsd_data/crane.definition\n",
      "Reading wsd_data/motion.definition\n",
      "Reading wsd_data/palm.definition\n",
      "Reading wsd_data/plant.definition\n",
      "Reading wsd_data/tank.definition\n",
      "Generated definitions! Number of definitions stored is 12\n"
     ]
    }
   ],
   "source": [
    "# Use an OrderedDict to remember the order of the keys\n",
    "# The senses of each term is currently in order of their usage in WordNet\n",
    "# So vital to keep the order maintained in the dictionary, as this will be used\n",
    "# while tie-breaking\n",
    "def_dict = OrderedDict()\n",
    "last_key = None\n",
    "for dfile in def_files:\n",
    "    print(\"Reading\",dfile)\n",
    "    with open(dfile,encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#DEFINITION\"):\n",
    "                defs = line.split()[1]\n",
    "                def_w, def_m = defs.split('%')[0], defs.split('%')[1]\n",
    "                last_key = (def_w,def_m)\n",
    "                #print(\"Last key\",last_key)\n",
    "                def_dict[last_key] = None\n",
    "            # Useless considering a line that is just a newline\n",
    "            elif line != \"\\n\":\n",
    "                #print(line)\n",
    "                norm_line = normalize_text(line)\n",
    "                #print(\"Normed\",normalize_text(line))\n",
    "                norm_line = set(norm_line)\n",
    "                # The word itself shouldn't be in the set of words defining it\n",
    "                norm_line.remove(last_key[0])\n",
    "                def_dict[last_key] = (norm_line)\n",
    "#print(\"DEFINITION DICTIONARY\",def_dict.keys())\n",
    "print(\"Generated definitions! Number of definitions stored is\",len(def_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorensen_similarity(set_x, set_y):\n",
    "    intersect = set_x.intersection(set_y)\n",
    "    #print(\"Intersection is\",intersect)\n",
    "    #union = set_x.union(set_y)\n",
    "    score = (2 * len(intersect)) / (len(set_x) + len(set_y))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(set_x, set_y):\n",
    "    intersect = set_x.intersection(set_y)\n",
    "    union = set_x.union(set_y)\n",
    "    score = len(intersect) / len(union)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_word_sense(test_text, definitions,similarity_measure=\"sorensen\"):\n",
    "    max_score = -100000000\n",
    "    predicted_sense = None\n",
    "    for definition in definitions:\n",
    "        def_word = definition[0]\n",
    "        def_sense = definition[1]\n",
    "        # Word for which we are trying to predict the sense should not be in the set of test words\n",
    "        try:\n",
    "            assert def_word not in test_text\n",
    "        except Exception as e:\n",
    "            #print(def_word)\n",
    "            #print(test_text)\n",
    "            raise e\n",
    "        # Get similarity score\n",
    "        #print(\"Using definition\",definition)\n",
    "        if similarity_measure == \"sorensen\":\n",
    "            score = sorensen_similarity(test_text,definitions[definition])\n",
    "        else:\n",
    "            score = jaccard_similarity(test_text,definitions[definition])\n",
    "        if score > max_score:\n",
    "            #print(\"Score is\",score)\n",
    "            max_score = score\n",
    "            predicted_sense = def_sense\n",
    "    return (predicted_sense,max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading wsd_data/bass.test\n",
      "Number of cases 107\n",
      "Reading wsd_data/crane.test\n",
      "Number of cases 95\n",
      "Reading wsd_data/motion.test\n",
      "Number of cases 201\n",
      "Reading wsd_data/palm.test\n",
      "Number of cases 201\n",
      "Reading wsd_data/plant.test\n",
      "Number of cases 188\n",
      "Reading wsd_data/tank.test\n",
      "Number of cases 201\n"
     ]
    }
   ],
   "source": [
    "# Time to read the test files\n",
    "true_labels = [] # List of list... Each inner list has list of tuples that has the term and true sense\n",
    "test_texts = [] # List of lists... Inner list is of sets of test texts\n",
    "\n",
    "for tfile in test_files:\n",
    "    #true_label = None\n",
    "    true_label = []\n",
    "    test_text = []\n",
    "    set_text = []\n",
    "    print(\"Reading\",tfile)\n",
    "    with open(tfile,encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#LABEL\"):\n",
    "                labs = line.split()[1]\n",
    "                #true_label = (labs.split('%')[0], labs.split('%')[1])\n",
    "                lab = (labs.split('%')[0], labs.split('%')[1])\n",
    "                true_label.append(lab)\n",
    "                # TODO\n",
    "                #true_labels.append(true_label)\n",
    "                if len(set_text) != 0:\n",
    "                    test_text.append(set(set_text))\n",
    "                    set_text = []\n",
    "            elif line != \"\\n\":\n",
    "                #print(line)\n",
    "                norm_line = normalize_text(line)\n",
    "                #print(\"Normed\",normalize_text(line))\n",
    "                norm_line = set(norm_line)\n",
    "                # The word itself shouldn't be in the set of words\n",
    "                #print(\"True label\")\n",
    "                norm_line.remove(lab[0])\n",
    "                set_text += norm_line\n",
    "    # This is to get the body of the last test text, as we will miss it above\n",
    "    test_text.append(set(set_text))\n",
    "    set_text = []\n",
    "    test_texts.append((test_text))\n",
    "    test_text = []\n",
    "    true_labels.append(true_label)\n",
    "    #last_count = len(true_labels[-2]) if len(true_labels)>1 else 0\n",
    "    #print(last_count)\n",
    "    print(\"Number of cases\",len(true_labels[-1]))\n",
    "#print(len(test_texts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************Testing for wsd_data/bass.test *************\n",
      "\n",
      "Accuracy achieved for word bass is 75.70093457943925 %\n",
      "\n",
      "\n",
      "Accuracy achieved for word bass using Jaccard similarity is 75.70093457943925 %\n",
      "\n",
      "\n",
      "By just guessing accuracy achieved for word bass is 90.65420560747664 %\n",
      "\n",
      "*************Testing for wsd_data/crane.test *************\n",
      "\n",
      "Accuracy achieved for word crane is 80.0 %\n",
      "\n",
      "\n",
      "Accuracy achieved for word crane using Jaccard similarity is 80.0 %\n",
      "\n",
      "\n",
      "By just guessing accuracy achieved for word crane is 75.78947368421053 %\n",
      "\n",
      "*************Testing for wsd_data/motion.test *************\n",
      "\n",
      "Accuracy achieved for word motion is 66.66666666666666 %\n",
      "\n",
      "\n",
      "Accuracy achieved for word motion using Jaccard similarity is 66.66666666666666 %\n",
      "\n",
      "\n",
      "By just guessing accuracy achieved for word motion is 70.64676616915423 %\n",
      "\n",
      "*************Testing for wsd_data/palm.test *************\n",
      "\n",
      "Accuracy achieved for word palm is 78.1094527363184 %\n",
      "\n",
      "\n",
      "Accuracy achieved for word palm using Jaccard similarity is 78.1094527363184 %\n",
      "\n",
      "\n",
      "By just guessing accuracy achieved for word palm is 71.14427860696517 %\n",
      "\n",
      "*************Testing for wsd_data/plant.test *************\n",
      "\n",
      "Accuracy achieved for word plant is 60.63829787234043 %\n",
      "\n",
      "\n",
      "Accuracy achieved for word plant using Jaccard similarity is 60.63829787234043 %\n",
      "\n",
      "\n",
      "By just guessing accuracy achieved for word plant is 54.25531914893617 %\n",
      "\n",
      "*************Testing for wsd_data/tank.test *************\n",
      "\n",
      "Accuracy achieved for word tank is 52.23880597014925 %\n",
      "\n",
      "\n",
      "Accuracy achieved for word tank using Jaccard similarity is 52.23880597014925 %\n",
      "\n",
      "\n",
      "By just guessing accuracy achieved for word tank is 37.3134328358209 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(true_labels)):\n",
    "    true_label = true_labels[i]\n",
    "    test_text = test_texts[i]\n",
    "    print(\"*************Testing for\",test_files[i],\"*************\")\n",
    "    correct_predictions_sorensen = 0\n",
    "    correct_predictions_jaccard = 0\n",
    "    # Get the corresponding definitions and store in an abriged dictionary\n",
    "    abriged_dict = OrderedDict()\n",
    "    for key in def_dict:\n",
    "        if true_label[0][0] in key:\n",
    "            abriged_dict[key] = def_dict[key]\n",
    "    #print(\"Keys:\",abriged_dict.keys())\n",
    "    # If we guess, we guess every class to be the one with the highest WordNet frequency\n",
    "    guess = list(abriged_dict.keys())[0][1]\n",
    "    guess_predictions = 0\n",
    "    for j in range(len(true_label)):\n",
    "        true_lab = true_label[j]\n",
    "        text = test_text[j]\n",
    "        \n",
    "        #print(\"Truth label is\",true_lab)\n",
    "        pred_sorensen = predict_word_sense(text,abriged_dict,similarity_measure=\"sorensen\")\n",
    "        pred_jaccard = predict_word_sense(text,abriged_dict,similarity_measure=\"jaccard\")\n",
    "        #print(j+1,\"Prediction of\",true_lab[0],\"is\",pred[0],\"while true label is\",true_lab[1])\n",
    "        if pred_sorensen[0] == true_lab[1]:\n",
    "            correct_predictions_sorensen += 1\n",
    "        if pred_jaccard[0] == true_lab[1]:\n",
    "            correct_predictions_jaccard += 1\n",
    "        if guess == true_lab[1]:\n",
    "            guess_predictions += 1\n",
    "    accuracy_sorensen = (correct_predictions_sorensen / len(true_label))\n",
    "    accuracy_jaccard = (correct_predictions_jaccard / len(true_label))\n",
    "    guess_accuracy = (guess_predictions / len(true_label))\n",
    "    print(\"\\nAccuracy achieved for word\",true_lab[0],\"is\",accuracy_sorensen*100,\"%\\n\")\n",
    "    print(\"\\nAccuracy achieved for word\",true_lab[0],\"using Jaccard similarity is\",accuracy_jaccard*100,\"%\\n\")\n",
    "    print(\"\\nBy just guessing accuracy achieved for word\",true_lab[0],\"is\",guess_accuracy*100,\"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. How this algorithm differs from the original algorithm\n",
    "<p>The original Lesk's algorithm actually used the lexicon definitions of the context words and the description of the target word to classify the sense of a particular word. So, we used the different senses of the target word and also the lexicographic meanings of the context words. </p>\n",
    "<p>However, here in the simplified Lesk's algorithm, we use a bag of words model, and only take into account which words appear in the definition of a word in a dictionary or thesaurus. Then using this set or bag of words, we check for overlap with a test text using a suitable similarity measure. Hence we use no context lexicons.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
