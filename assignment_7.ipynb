{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = \"Materials/\"\n",
    "train_path = os.path.join(corpus_path,\"train/\")\n",
    "test_path = os.path.join(corpus_path,\"test/\")\n",
    "stopword_path = os.path.join(corpus_path,\"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes are: ['Biology', 'Chemistry', 'Physics']\n"
     ]
    }
   ],
   "source": [
    "train_files_list = os.listdir(train_path)\n",
    "NUM_CLASSES = len(train_files_list)\n",
    "NUM_FEATURES = 10\n",
    "CLASSES = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    CLASSES.append(train_files_list[i].split('.')[0])\n",
    "print(\"Classes are:\",CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file Biology.txt\n",
      "Reading file Chemistry.txt\n",
      "Reading file Physics.txt\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for file in train_files_list:\n",
    "    train_file = os.path.join(train_path,file)\n",
    "    print(\"Reading file\",file)\n",
    "    with open(train_file,encoding='utf8') as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stopwords we remove\n",
      " ['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along']\n"
     ]
    }
   ],
   "source": [
    "with open(stopword_path,encoding='utf8') as s_f:\n",
    "    stopwords = s_f.read().split()\n",
    "print(\"Some stopwords we remove\\n\",stopwords[:20])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(texts):\n",
    "    norm_texts = []\n",
    "    for text in texts:\n",
    "        # Lower case\n",
    "        text = text.lower()\n",
    "        list_words = text.split()\n",
    "        words_wo_stop = []\n",
    "        # Stop word Removal\n",
    "        for word in list_words:\n",
    "            if word not in stopwords:\n",
    "                # Lemmatization\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "                # Stemming\n",
    "                stemmer = SnowballStemmer(\"english\")\n",
    "                word = stemmer.stem(word)\n",
    "                words_wo_stop.append(word)\n",
    "        text = \" \".join(words_wo_stop)\n",
    "        # Remove punctuation and numbers\n",
    "        text = re.sub('['+string.punctuation+']', ' ', text)\n",
    "        text = re.sub('[·´]','',text)\n",
    "        text = re.sub('[0-9]','',text)\n",
    "        \n",
    "        norm_texts.append(text.split())\n",
    "    return norm_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_texts = normalize_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foreword', 'preserv', 'anim', 'plant', 'life', 'general', 'beauti', 'nature', 'foremost', 'duti', 'men', 'woman', 'to', 'day', 'imper', 'duty', 'perform', 'once', 'late', 'mean', 'preservation', 'sentimental', 'educ', 'legislative', 'must', 'employed', 'present', 'warn', 'issu', 'uncertain', 'sound', 'great', 'battl', 'preserv', 'conserv', 'won', 'gentl', 'tones', 'appeal', 'aesthet', 'instinct', 'sens', 'beauty', 'enjoy', 'nature', 'sound', 'loud', 'alarm', 'present', 'fact', 'strong', 'language', 'back', 'irrefut', 'statist', 'photograph', 'lies', 'establish', 'law', 'enforc', 'bludgeon', 'book', 'alarm', 'call', 'forc', 'page', 'remind', 'sound', 'great', 'bell', 'watch', 'tow', 'citi', 'middl', 'age', 'call', 'citizen', 'arm', 'protect', 'homes', 'liberti', 'happiness', 'undeni', 'welfar', 'happi', 'futur', 'generat', 'american', 'stake', 'battl', 'preserv', 'natur', 'selfishness', 'ignorance', 'cruelti', 'destroyers', 'longer', 'destroy', 'great', 'work']\n",
      "Length of Document 0 is 88903\n",
      "Length of Document 1 is 57676\n",
      "Length of Document 2 is 42308\n",
      "Min Length = 42308\n"
     ]
    }
   ],
   "source": [
    "print(normalized_texts[0][:100])\n",
    "min_length = 1000000000000\n",
    "for i in range(NUM_CLASSES):\n",
    "    if len(normalized_texts[i]) < min_length:\n",
    "        min_length = len(normalized_texts[i])\n",
    "    print(\"Length of Document\",i,\"is\",len(normalized_texts[i]))\n",
    "print(\"Min Length =\",min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRUNCATE_FILES = False\n",
    "if TRUNCATE_FILES:\n",
    "    normalized_texts = [text[:min_length+1] for text in normalized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_counts(texts):\n",
    "    counts_list = []\n",
    "    for text in texts:\n",
    "        uni_counts = {}\n",
    "        txt_list = text\n",
    "        for w in txt_list:\n",
    "            if w not in uni_counts:\n",
    "                uni_counts[w] = 1\n",
    "            else:\n",
    "                uni_counts[w] += 1\n",
    "        counts_list.append(uni_counts)\n",
    "    return counts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_dicts = get_counts(normalized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of doc: 88903\n",
      "Length of doc: 57676\n",
      "Length of doc: 42308\n",
      "14150\n",
      "188887\n",
      "14150\n"
     ]
    }
   ],
   "source": [
    "# Generate vocabulary\n",
    "vocabulary = []\n",
    "N_train = 0\n",
    "counts_train = {}\n",
    "for count_dict in counts_dicts:\n",
    "    vocabulary = vocabulary + list(count_dict.keys())\n",
    "    print(\"Length of doc:\",sum(list(count_dict.values())))\n",
    "    N_train += sum(list(count_dict.values()))\n",
    "    for w in count_dict:\n",
    "        if w not in counts_train:\n",
    "            counts_train[w] = count_dict[w]\n",
    "        else:\n",
    "            counts_train[w] += count_dict[w]\n",
    "vocabulary = set(vocabulary)\n",
    "print(len(vocabulary))\n",
    "print(N_train)\n",
    "print(len(counts_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_probs(train_1,vocabulary,smooth=False,alpha=0.0,N=None,V=None):\n",
    "    #vocabs = []\n",
    "    probs_1 = {}\n",
    "    assert N is not None\n",
    "    if smooth:\n",
    "        assert V is not None\n",
    "    for word in vocabulary:\n",
    "        if word in train_1:\n",
    "    #for word in train_1:\n",
    "            if not smooth:\n",
    "                #probs_1 = {k:v/N for k,v in train_1.items()}\n",
    "                probs_1[word] = train_1[word] / N\n",
    "            else:\n",
    "                probs_1[word] = (train_1[word] + alpha) / (N + alpha * V)\n",
    "        else:\n",
    "            if not smooth:\n",
    "            #probs_1 = {k:v/N for k,v in train_1.items()}\n",
    "                probs_1[word] = 0.0\n",
    "            else:\n",
    "                probs_1[word] = (0.0 + alpha) / (N + alpha * V)\n",
    "    return probs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PMI Computation\n",
    "pmi_list = []\n",
    "train_prob_dicts = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    pmi_dict = {}\n",
    "    # Get counts\n",
    "    count_dict = counts_dicts[i]\n",
    "    N = sum(list(count_dict.values()))\n",
    "    # Generate the probabilities and store them\n",
    "    prob_dict = generate_probs(count_dict,list(count_dict.keys()),N=N,smooth=False)\n",
    "    #prob_dict = generate_probs(count_dict,vocabulary,N=N,smooth=True,V=len(vocabulary),alpha=0.1)\n",
    "    train_prob_dicts.append(prob_dict)\n",
    "    for word in prob_dict:\n",
    "        try:\n",
    "            pmi_dict[word] = math.log(prob_dict[word] / (counts_train[word] / N_train))\n",
    "        except:\n",
    "            # Should never come here\n",
    "            print(word,prob_dict[word],counts_train[word],N_train)\n",
    "    pmi_list.append(pmi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arrange by class\n",
    "class_term_pmis = []\n",
    "for i in range(1,NUM_CLASSES+1):\n",
    "    class_term_pmis.append([])\n",
    "for word in vocabulary:\n",
    "    max_val = -10000000\n",
    "    max_term = None\n",
    "    max_class = None\n",
    "    for k in range(1,NUM_CLASSES+1):\n",
    "        pmi = pmi_list[k-1]\n",
    "        if word in pmi:\n",
    "            if pmi[word] > max_val:\n",
    "                max_val = pmi[word]\n",
    "                max_term = word\n",
    "                max_class = k\n",
    "    class_term_pmis[max_class-1].append((max_term,max_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biology.txt\n",
      "Chemistry.txt\n",
      "Physics.txt\n",
      "List of features:\n",
      " [['deck', 'barwonleigh', 'colonist', 'rupe', 'gist', 'lust', 'thomas', 'disputed', 'chestnut', 'waigiou'], ['bacl', 'arsenious', 'rutherford', 'botanist', 'reduction', 'cacl', 'coinag', 'recovered', 'ccl', 'elements'], ['accommodated', 'jointly', 'ebullition', 'quaver', 'inexplicable', 'venus', 'benefactor', 'abxv', 'jupit', 'conchoid']]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 of each\n",
    "pmi_features_per_class = []\n",
    "for c in range(1,NUM_CLASSES+1):\n",
    "    pmis = class_term_pmis[c-1]\n",
    "    print(train_files_list[c-1])\n",
    "    sorted_by_second = sorted(pmis, key=lambda tup: tup[1],reverse=True)\n",
    "    top_10 = sorted_by_second[:NUM_FEATURES]\n",
    "    #print(top_10)\n",
    "    #print(\"*************************************************\")\n",
    "    list_feats = [x[0] for x in top_10]\n",
    "    #pmi_features_per_class += (list_feats)\n",
    "    pmi_features_per_class.append(list_feats)\n",
    "print(\"List of features:\\n\",pmi_features_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MI Computation\n",
    "P_class = 1 / NUM_CLASSES\n",
    "mi_list = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    mi_dict = {}\n",
    "    # Get counts\n",
    "    count_dict = counts_dicts[i]\n",
    "    N = sum(list(count_dict.values()))\n",
    "    prob_dict = train_prob_dicts[i]\n",
    "    for word in prob_dict:\n",
    "        try:\n",
    "            # Computation changes!!!\n",
    "            mi_dict[word] = (prob_dict[word] * P_class) * (math.log((prob_dict[word]) / (counts_train[word] / N_train)))\n",
    "            #mi_dict[word] = (prob_dict[word]) * (math.log((prob_dict[word]) / (counts_train[word] / N_train)))\n",
    "        except:\n",
    "            # Should never come here\n",
    "            print(word,prob_dict[word],counts_train[word],N_train)\n",
    "    mi_list.append(mi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mi_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arrange by class\n",
    "class_term_mis = []\n",
    "for i in range(1,NUM_CLASSES+1):\n",
    "    class_term_mis.append([])\n",
    "for word in vocabulary:\n",
    "    max_val = -10000000\n",
    "    max_term = None\n",
    "    max_class = None\n",
    "    for k in range(1,NUM_CLASSES+1):\n",
    "        mi = mi_list[k-1]\n",
    "        if word in mi:\n",
    "            if mi[word] > max_val:\n",
    "                max_val = mi[word]\n",
    "                max_term = word\n",
    "                max_class = k\n",
    "    class_term_mis[max_class-1].append((max_term,max_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biology.txt\n",
      "Chemistry.txt\n",
      "Physics.txt\n",
      "List of features: [['game', 'bird', 'wild', 'state', 'kill', 'law', 'year', 'protect', 'deer', 'life'], ['acid', 'form', 'h', 'oxygen', 'water', 'o', 'hydrogen', 'carbon', 'gas', 'element'], ['colour', 'light', 'ray', 'refract', 'd', 'prism', 'glass', 'red', 'part', 'distanc']]\n"
     ]
    }
   ],
   "source": [
    "# Top 10 of each\n",
    "mi_features_per_class = []\n",
    "for c in range(1,NUM_CLASSES+1):\n",
    "    mis = class_term_mis[c-1]\n",
    "    print(train_files_list[c-1])\n",
    "    sorted_by_second = sorted(mis, key=lambda tup: tup[1],reverse=True)\n",
    "    top_10 = sorted_by_second[:NUM_FEATURES]\n",
    "    #print(top_10)\n",
    "    #print(\"*************************************************\")\n",
    "    list_feats = [x[0] for x in top_10]\n",
    "    #mi_features_per_class += (list_feats)\n",
    "    mi_features_per_class.append(list_feats)\n",
    "print(\"List of features:\",mi_features_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file Materials/test/test1.txt\n",
      "Reading file Materials/test/test2.txt\n",
      "Reading file Materials/test/test3.txt\n",
      "Reading file Materials/test/test4.txt\n",
      "Reading file Materials/test/test5.txt\n",
      "Reading file Materials/test/test6.txt\n"
     ]
    }
   ],
   "source": [
    "# Read test data\n",
    "test_files_list = os.listdir(test_path)\n",
    "test_texts = []\n",
    "for file in test_files_list:\n",
    "    test_file = os.path.join(test_path,file)\n",
    "    print(\"Reading file\",test_file)\n",
    "    with open(test_file,encoding='utf8') as f:\n",
    "        test_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize test texts\n",
    "normalized_test_texts = normalize_text(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(NUM_CLASSES):\n",
    "#     print(\"Printing features\")\n",
    "#     feature = mi_features_per_class[i]\n",
    "#     for f in feature:\n",
    "#         print(\"Count of\",f,\"is\",counts_dicts[i][f])\n",
    "#         print(\"MLE of\",f,\"is\",train_prob_dicts[i][f])\n",
    "# print(\"----------------------------------------\")\n",
    "# for text in normalized_test_texts:\n",
    "#     print(\"Text\")\n",
    "#     for i in range(NUM_CLASSES):\n",
    "#         feature = mi_features_per_class[i]\n",
    "#         f_sum = 0\n",
    "#         for f in feature:\n",
    "#             print(\"Feature\",f,\"count\",text.count(f))\n",
    "#             f_sum += text.count(f)\n",
    "#         print(\"Tot Count\",f_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffcontent', 'eha', 'foot', 'hand', 'ii', 'bill', 'bird', 'iii', 'tail', 'iv', 'nose', 'ear', 'vi', 'tommi', 'vii', 'barn', 'owl', 'viii', 'domest', 'anim', 'ix', 'snake', 'indian', 'snake', 'charm', 'xi', 'cure', 'snake', 'bit', 'xii', 'cobra', 'bungalow', 'xiii', 'panther', 'shoot', 'xiv', 'purbhoo', 'xv', 'coconut', 'tree', 'xvi', 'betel', 'nut', 'xvii', 'hindu', 'festiv', 'xviii', 'indian', 'poverti', 'xix', 'borrow', 'indian', 'word', 'special', 'due', 'editor', 'proprietor', 'strand', 'magazine', 'pall', 'mall', 'magazine', 'time', 'india', 'courtesi', 'permit', 'reprint', 'articl', 'book', 'origin', 'appear', 'columns', 'list', 'illustr', 'half', 'ton', 'eha', 'nose', 'eleph', 'hand', 'redeem', 'mind', 'good', 'rough', 'job', 'competit', 'keen', 'rat', 'relat', 'squirrel', 'zoolog', 'person', 'gutter', 'snipe', 'tail', 'drag', 'dirti', 'rope', 'blackbird', 'starling']\n"
     ]
    }
   ],
   "source": [
    "print(normalized_test_texts[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_test_dicts = get_counts(normalized_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "unk_prob = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    count_dict = counts_dicts[i]\n",
    "    N = sum(list(count_dict.values()))\n",
    "    unk_prob.append((0.0 + alpha) / (N + alpha * len(vocabulary)))\n",
    "    #unk_prob.append((0.0 + alpha) / (N + alpha * len(vocabulary)))\n",
    "#print(unk_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Using PMI Features**********\n",
      "Predicted class of document is 1 that is Chemistry\n",
      "Predicted class of document is 0 that is Biology\n",
      "Predicted class of document is 2 that is Physics\n",
      "Predicted class of document is 2 that is Physics\n",
      "Predicted class of document is 0 that is Biology\n",
      "Predicted class of document is 0 that is Biology\n",
      "**********Using MI Features**********\n",
      "Predicted class of document is 1 that is Chemistry\n",
      "Predicted class of document is 2 that is Physics\n",
      "Predicted class of document is 2 that is Physics\n",
      "Predicted class of document is 0 that is Biology\n",
      "Predicted class of document is 1 that is Chemistry\n",
      "Predicted class of document is 1 that is Chemistry\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Naive Bayes Classifier\n",
    "0 --> Biology\n",
    "1 --> Chemistry\n",
    "2 --> Physics\n",
    "'''\n",
    "for features in [pmi_features_per_class,mi_features_per_class]:\n",
    "    preds = []\n",
    "    if features == mi_features_per_class:\n",
    "        print(\"**********Using MI Features**********\")\n",
    "    else:\n",
    "        print(\"**********Using PMI Features**********\")\n",
    "    #features = pmi_features_per_class\n",
    "    for text in normalized_test_texts:\n",
    "        #vocab_test = list(set(text))\n",
    "        vocab_test = text\n",
    "        #print(\"Test vocab size:\",len(vocab_test))\n",
    "        prob_class = []\n",
    "        '''\n",
    "        Compute log probabilities. Then max_prob = max_log(prob)\n",
    "        '''\n",
    "        all_features = []\n",
    "        for f in features:\n",
    "            all_features += f\n",
    "        for i in range((NUM_CLASSES)):\n",
    "            log_prob = math.log(P_class)\n",
    "            #print(\"LOG P_class:\",log_prob)\n",
    "            for token in vocab_test:\n",
    "                #print(\"Features are:\",features[i])\n",
    "                #if token in features[i] and token in train_prob_dicts[i]:\n",
    "                if token in all_features:\n",
    "                    #print(\"Here\")\n",
    "                    if token in train_prob_dicts[i]:\n",
    "                        log_prob += math.log(train_prob_dicts[i][token])\n",
    "                    #else:\n",
    "                    #    log_prob += math.log(unk_prob[i])\n",
    "                #else:\n",
    "                #    log_prob += math.log(unk_prob[i])\n",
    "            prob_class.append(log_prob)\n",
    "        #print(\"Probs:\",prob_class)\n",
    "        preds.append(prob_class.index(max(prob_class)))\n",
    "\n",
    "    '''\n",
    "    Now print the predictions\n",
    "    '''\n",
    "    for i in range(len(test_files_list)):\n",
    "        print(\"Predicted class of document is\",preds[i],\"that is\",CLASSES[preds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
